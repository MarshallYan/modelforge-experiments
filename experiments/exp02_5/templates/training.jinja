[training]
number_of_epochs = 500
remove_self_energies = false
batch_size = 64
lr = 1e-4
monitor = "val/per_system_energy/rmse"
shift_center_of_mass_to_origin = true
gradient_clip_val = 5

[training.experiment_logger]
logger_name = "wandb"
[training.experiment_logger.wandb_configuration]
save_dir = "logs"
project = "{{ project }}"
group = "{{ group }}"
log_model = true
job_type = "benchmarking"
tags = {{ tags }}
notes = "Train the best SchNet QM9 we can"

[training.lr_scheduler]
scheduler_name = "ReduceLROnPlateau"
frequency = 1
mode = "min"
factor = 0.1
patience = 10
cooldown = 5
min_lr = 1e-8
threshold = 0.1
threshold_mode = "abs"
interval = "epoch"

[training.loss_parameter]
loss_components = ['per_system_energy', 'per_system_dipole_moment', "per_atom_charge", "per_atom_force"]

[training.loss_parameter.weight]
per_system_energy = 1 #NOTE: reciprocal units
per_system_dipole_moment = 0 #NOTE: reciprocal units
#per_system_total_charge = 0.1
per_atom_charge = 0
per_atom_force = {{ per_atom_force }}


#[training.loss_parameter.target_weight]
#per_system_dipole_moment = 1
#per_atom_charge = 1
#per_atom_force = 0.00001

#[training.loss_parameter.mixing_steps]
#per_system_dipole_moment= 0.005
#per_atom_charge = 0.005
#per_atom_force = 0.0000001

[training.early_stopping]
verbose = true
min_delta = 0.001
patience = 50

[training.splitting_strategy]
name = "random_record_splitting_strategy"
data_split = [0.8, 0.1, 0.1]
seed = {{ seed }}
